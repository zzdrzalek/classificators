# -*- coding: utf-8 -*-
"""Zdrzalek_Skolimowski.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YwflmeQCWn97GU-fD4IsqjmSMVbSwe7K
"""

import pandas as pd
df = pd.read_csv('listings.csv')

#Dataset
df = df[['review_scores_rating', 'price', 'room_type', 'beds', 'bedrooms', 'host_is_superhost', 'host_has_profile_pic']]
df.head()

df.shape

df = df.dropna()
df.replace(to_replace = 'f', value = 0, inplace = True)
df.replace(to_replace = 't', value = 1, inplace = True)
df['price'] = df['price'].str.replace('$', '')
df['price'] = df['price'].str.replace(',', '')
df["price"] = df.price.astype(float)

df.shape

df["room_type"].unique()

df["price"].quantile(0.3)

df["price"].quantile(0.6)

df = pd.get_dummies(df)

df.columns

price_class = []
price_class_l = []
for v in df["price"]:
    if v < 693:
        price_class.append("tani")
        price_class_l.append(1)
    elif 693 <= v < 967:
        price_class.append("sredni")
        price_class_l.append(2)
    elif v >= 967:
        price_class.append("drogi")
        price_class_l.append(3)
df["price_class"] = price_class

df.info()

# LOGISTIC REGRESSION

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
import seaborn as sn
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, f1_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import precision_score, recall_score, confusion_matrix
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

X = df[['review_scores_rating', 'beds', 'bedrooms',
       'host_is_superhost', 'host_has_profile_pic',
       'room_type_Entire home/apt', 'room_type_Hotel room',
       'room_type_Private room', 'room_type_Shared room']]
y = df['price_class']

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=10)

logistic_regression=LogisticRegression(multi_class="multinomial", C=1)
logistic_regression.fit(X_train,y_train)
#predictions = logmodel.predict(X_test)
#print(X_train)

#score = cross_val_score(logmodel, X_train, y_train, cv=5)
#score.mean()

grid_param = {'penalty' : ['l1', 'l2'], 'C':np.logspace(0.1,10,20), 'max_iter': [1000]}
grid_search_cv = GridSearchCV(LogisticRegression(multi_class="multinomial"), grid_param, n_jobs=-1, verbose =1,scoring='accuracy')
grid_search_cv.fit(X_train, y_train)

grid_search_cv.score(X_test, y_test)
#grid_search_cv.best_params_

logistic_regression.score(X_test, y_test)

logistic_regression= LogisticRegression(multi_class="multinomial")
logistic_regression.fit(X_train,y_train)
y_pred=logistic_regression.predict(X_test)

print('Accuracy: ',metrics.accuracy_score(y_test, y_pred))

import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

model = LogisticRegression(solver='liblinear', random_state=10)

from sklearn.pipeline import Pipeline

pipe = Pipeline([('classifier' , LogisticRegression(multi_class="multinomial"))])

# NAIWNY KLASYFIKATOR BAYESA

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
gnb.fit(X_train, y_train)
gnb.score(X_test, y_test)

nb_classifier = MultinomialNB()
params_NB = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0, ]}
gs_NB = GridSearchCV(nb_classifier, 
                 param_grid=params_NB,   # use any cross validation technique 
                 verbose=-1, 
                 n_jobs=-1,
                 scoring='accuracy') 
gs_NB.fit(X_train, y_train)
y_pred_NB = gs_NB.predict(X_test)

gs_NB.score(X_test, y_test)

# SVC

from sklearn.model_selection import GridSearchCV
from sklearn import svm
from sklearn.svm import SVC

classifier = SVC(kernel='linear', random_state = 10290)
classifier.fit(X_train, y_train)
print(classifier.score(X_test, y_test))

clf = SVC() 
#parameters 
param_grid = {'C': [0.1, 1, 10, 100, 1000],
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']}
grid_SVC = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)
SVM_BEST = grid_SVC.fit(X_train, y_train)
y_pred_SVM = grid_SVC.predict(X_test)
print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred_SVM)))
print('Precision Score : ' + str(precision_score(y_test,y_pred_SVM)))
print('Recall Score : ' + str(recall_score(y_test,y_pred_SVM)))
print('F1 Score : ' + str(f1_score(y_test,y_pred_SVM)))
#SVMConfusion matrix
confusion_matrix(y_test,y_pred_SVM)

SVM_BEST.score(X_test,y_test)

# decision tree

from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=0)
clf.fit(X_train, y_train)
clf.score(X_test, y_test)

# random forest

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(random_state=0)
clf.fit(X_train, y_train)
clf.score(X_test, y_test)

forest = RandomForestClassifier(random_state = 42)
n_estimators = [100, 300, 500, 800, 1200]
max_depth = [5, 8, 15, 25, 30]
min_samples_split = [2, 5, 10, 15, 100]
min_samples_leaf = [1, 2, 5, 10] 

hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  
              min_samples_split = min_samples_split, 
             min_samples_leaf = min_samples_leaf)

gridF = GridSearchCV(forest, hyperF, cv = 3, verbose = -1, 
                      n_jobs = -1)
bestForest = gridF.fit(X_train, y_train)
y_pred_forest = gridF.predict(X_test)

bestForest.score(X_test, y_test)

# wiekszosciowy

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
clf1 = LogisticRegression(multi_class='multinomial', random_state=1)
clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
clf3 = GaussianNB()

eclf1 = VotingClassifier(estimators=[
        ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
eclf1 = eclf1.fit(X_train, y_train)

eclf2 = VotingClassifier(estimators=[
        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
        voting='soft')
eclf2 = eclf2.fit(X_train, y_train)

eclf3 = VotingClassifier(estimators=[
       ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
       voting='soft', weights=[2,1,1],
       flatten_transform=True)
eclf3 = eclf3.fit(X_train, y_train)

eclf1.score(X_test, y_test)

eclf2.score(X_test, y_test)

eclf3.score(X_test, y_test)

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
clf1 = grid_search_cv
clf2 = bestForest
clf3 = gs_NB

eclf1 = VotingClassifier(estimators=[
        ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')
eclf1 = eclf1.fit(X_train, y_train)

eclf2 = VotingClassifier(estimators=[
        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
        voting='soft')
eclf2 = eclf2.fit(X_train, y_train)

eclf3 = VotingClassifier(estimators=[
       ('lr', clf1), ('rf', clf2), ('gnb', clf3)],
       voting='soft', weights=[2,1,1],
       flatten_transform=True)
eclf3 = eclf3.fit(X_train, y_train)

eclf1.score(X_test, y_test)

eclf2.score(X_test, y_test)

eclf3.score(X_test, y_test)